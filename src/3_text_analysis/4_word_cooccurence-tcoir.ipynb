{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tCoIR - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import common.utility as utility\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display #, set_matplotlib_formats\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import domain_logic_tCoIR as domain_logic\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> HAL Co-Windows Ratio (CWR)<span style='float: right; color: red'>MANDATORY</span>\n",
    "\n",
    "\\begin{aligned}\n",
    "nw(x) &= \\text{number of sliding windows that contains term $x$} \\\\\n",
    "nw(x, y) &= \\text{number of sliding windows that contains $x$ and $y$} \\\\\n",
    "\\\\\n",
    "f(x, y) &= \\text{normalized version of nw(x, y)} \\\\\n",
    "CWR(x, y) &= \\frac{nw(x, y)}{nw(x) + nw(y) - nw(x, y)}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Term co-occurrence frequencies is calculated in accordance with Hyperspace Analogue to Language (Lund; Burgess, 1996) vector-space model. The computation is specified in detail in section 3.1 in (Chen; Lu, 2011).\n",
    "\n",
    "- Chen Z.; Lu Y., \"A Word Co-occurrence Matrix Based Method for Relevance Feedback\"\n",
    "- Lund, K.; Burgess, C. & Atchley, R. A. (1995). \"Semantic and associative priming in high-dimensional semantic space\".[Link](https://books.google.de/books?id=CSU_Mj07G7UC).\n",
    "- Lund, K.; Burgess, C. (1996). \"Producing high-dimensional semantic spaces from lexical co-occurrence\". doi:10.3758/bf03204766 [Link](https://dx.doi.org/10.3758%2Fbf03204766).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import glove\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    ''' Iterates corpus and add distict terms to vocabulary '''\n",
    "    logger.info('Builiding vocabulary...')\n",
    "    token2id = collections.defaultdict()\n",
    "    token2id.default_factory = token2id.__len__\n",
    "    for doc in corpus:\n",
    "        for term in doc:\n",
    "            token2id[term]\n",
    "    logger.info('Vocabulary of size {} built.'.format(len(token2id)))\n",
    "    return token2id\n",
    "\n",
    "# See http://www.foldl.me/2014/glove-python/\n",
    "class GloveVectorizer():\n",
    "    \n",
    "    def __init__(self, corpus=None, token2id=None):\n",
    "        \n",
    "        self.token2id = token2id\n",
    "        self._id2token = None\n",
    "        self.corpus = corpus        \n",
    "        \n",
    "    @property\n",
    "    def corpus(self):\n",
    "        return self._corpus\n",
    "    \n",
    "    @corpus.setter\n",
    "    def corpus(self, value):\n",
    "    \n",
    "        self._corpus = value\n",
    "        self.term_count = sum(map(len, value or []))\n",
    "        \n",
    "        if self.token2id is None and value is not None:\n",
    "            self.token2id = build_vocab(value)\n",
    "            self._id2token = None\n",
    "    \n",
    "    @property\n",
    "    def id2token(self):\n",
    "        if self._id2token is None:\n",
    "            if self.token2id is not None:\n",
    "                self._id2token = { v:k for k,v in self.token2id.items() }\n",
    "        return self._id2token\n",
    "    \n",
    "    #def fit(self, sentences, window=2, dictionary=None):\n",
    "    def fit(self, corpus=None, size=2):  #, distance_metric=0, zero_out_diag=False):\n",
    "        \n",
    "        if corpus is not None:\n",
    "            self.corpus = corpus\n",
    "            \n",
    "        assert self.token2id is not None, \"Fit with no vocabulary!\"\n",
    "        assert self.corpus is not None, \"Fit with no corpus!\"\n",
    "        \n",
    "        glove_corpus = glove.Corpus(dictionary=self.token2id)\n",
    "        glove_corpus.fit(corpus, window=size)\n",
    "\n",
    "        self.nw_xy = glove_corpus.matrix\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def cooccurence(self, normalize='size', zero_diagonal=True):\n",
    "        '''Return computed co-occurrence values'''\n",
    "        \n",
    "        matrix = self.nw_xy\n",
    "        \n",
    "        #if zero_diagonal:\n",
    "        #    matrix.fill_diagonal(0)\n",
    "                \n",
    "        coo_matrix = matrix #.tocoo(copy=False)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'x_id': coo_matrix.row,\n",
    "            'y_id': coo_matrix.col,\n",
    "            'nw_xy': coo_matrix.data,\n",
    "            'nw_x': 0,\n",
    "            'nw_y': 0,\n",
    "        }).reset_index(drop=True)\n",
    "        \n",
    "        df = df.assign(\n",
    "            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "            y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    "        )\n",
    "        \n",
    "        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y']]\n",
    "        \n",
    "        norm = 1.0\n",
    "        if normalize == 'size':\n",
    "            norm = self.term_count\n",
    "        elif normalize == 'max':\n",
    "            norm = np.max(coo_matrix)\n",
    "        elif normalize is None:\n",
    "            logger.warning('No normalize method specified. Using absolute counts...')\n",
    "            pass # return as as is...\"\n",
    "        else:\n",
    "            assert False, 'Unknown normalize specifier'\n",
    "\n",
    "        df_nw_xy = df.assign(cwr=df.nw_xy / norm)\n",
    "        \n",
    "        return df_nw_xy[df_nw_xy.cwr > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 11:26:40,542 : INFO : Builiding vocabulary...\n",
      "2019-06-25 11:26:40,543 : INFO : Vocabulary of size 7 built.\n",
      "2019-06-25 11:26:40,550 : INFO : Builiding vocabulary...\n",
      "2019-06-25 11:26:40,552 : INFO : Vocabulary of size 6 built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run OK\n"
     ]
    }
   ],
   "source": [
    "import sys, array, collections\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from glove import Corpus \n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import common.utility as utility\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "class HyperspaceAnalogueToLanguageVectorizer():\n",
    "    \n",
    "    def __init__(self, corpus=None, token2id=None, tick=utility.noop):\n",
    "        \"\"\"\n",
    "        Build vocabulary and create nw_xy term-term matrix and nw_x term global occurence vector\n",
    "        \n",
    "        Parameter:\n",
    "            corpus Iterable[Iterable[str]]\n",
    "\n",
    "        \"\"\"\n",
    "        self.token2id = token2id\n",
    "        self._id2token = None\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        self.nw_xy = None\n",
    "        self.nw_x = None\n",
    "        self.tick = tick\n",
    "\n",
    "    @property\n",
    "    def corpus(self):\n",
    "        return self._corpus\n",
    "    \n",
    "    @corpus.setter\n",
    "    def corpus(self, value):\n",
    "    \n",
    "        self._corpus = value\n",
    "        self.term_count = sum(map(len, value or []))\n",
    "        \n",
    "        if self.token2id is None and value is not None:\n",
    "            self.token2id = build_vocab(value)\n",
    "            self._id2token = None\n",
    "    \n",
    "    @property\n",
    "    def id2token(self):\n",
    "        if self._id2token is None:\n",
    "            if self.token2id is not None:\n",
    "                self._id2token = { v:k for k,v in self.token2id.items() }\n",
    "        return self._id2token\n",
    "    \n",
    "    def sliding_window(self, seq, n):\n",
    "        it = itertools.chain(iter(seq), [None] * n)\n",
    "        memory = tuple(itertools.islice(it, n+1))\n",
    "        if len(memory) == n+1:\n",
    "            yield memory\n",
    "        for x in it:\n",
    "            memory = memory[1:] + (x,)\n",
    "            yield memory\n",
    "\n",
    "    def fit(self, corpus=None, size=2, distance_metric=0, zero_out_diag=False):\n",
    "        \n",
    "        '''Trains HAL for a document. Note that sentence borders (for now) are ignored'''\n",
    "        \n",
    "        if corpus is not None:\n",
    "            self.corpus = corpus\n",
    "            \n",
    "        assert self.token2id is not None, \"Fit with no vocabulary!\"\n",
    "        assert self.corpus is not None, \"Fit with no corpus!\"\n",
    "\n",
    "        nw_xy = sp.lil_matrix ((len(self.token2id), len(self.token2id)), dtype=np.int32)\n",
    "        nw_x = np.zeros(len(self.token2id), dtype=np.int32)\n",
    "        \n",
    "        for terms in corpus:\n",
    "            \n",
    "            id_terms = ( self.token2id[size] for size in terms)\n",
    "            \n",
    "            self.tick()\n",
    "            \n",
    "            for win in self.sliding_window(id_terms, size):\n",
    "                \n",
    "                #logger.info([ self.id2token[x] if x is not None else None for x in win])\n",
    "                \n",
    "                if win[0] is None:\n",
    "                    continue\n",
    "                    \n",
    "                for x in win:\n",
    "                    if x is not None:\n",
    "                        nw_x[x] += 1\n",
    "\n",
    "                for i in range(1, size+1):\n",
    "\n",
    "                    if win[i] is None:\n",
    "                        continue\n",
    "                        \n",
    "                    if zero_out_diag:\n",
    "                        if win[0] == win[i]:\n",
    "                            continue\n",
    "                        \n",
    "                    d = float(i) # abs(n - i)\n",
    "                    if distance_metric == 0: #  linear i.e. adjacent equals window size, then decreasing by one\n",
    "                        w = (size - d + 1) # / size\n",
    "                    elif distance_metric == 1: # f(d) = 1 / d\n",
    "                        w = 1.0 / d\n",
    "                    elif distance_metric == 2: # Constant value of 1\n",
    "                        w = 1\n",
    "\n",
    "                    #print('*', i, self.id2token[win[0]], self.id2token[win[i]], w, [ self.id2token[x] if x is not None else None for x in win])\n",
    "                    nw_xy[win[0], win[i]] += w\n",
    "                    \n",
    "        self.nw_x = nw_x\n",
    "        self.nw_xy = nw_xy\n",
    "        #self.f_xy = nw_xy / np.max(nw_xy)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def to_df(self):\n",
    "        columns = [ self.id2token[i] for i in range(0,len(self.token2id))]\n",
    "        return pd.DataFrame(\n",
    "            data=self.nw_xy.todense(),\n",
    "            index=list(columns),\n",
    "            columns=list(columns),\n",
    "            dtype=np.float64\n",
    "        ).T\n",
    "    \n",
    "    def cwr(self, direction_sensitive=False, normalize='size'):\n",
    "\n",
    "        n = self.nw_x.shape[0]\n",
    "        \n",
    "        nw = self.nw_x.reshape(n,1)\n",
    "        nw_xy = self.nw_xy\n",
    "        \n",
    "        norm = 1.0\n",
    "        if normalize == 'size':\n",
    "            norm = float(self.term_count)\n",
    "        elif norm == 'max':\n",
    "            norm = float(np.max(nw_xy))\n",
    "        elif norm == 'sum':\n",
    "            norm = float(np.sum(nw_xy))\n",
    "        \n",
    "        #nw.resize(nw.shape[0], 1)\n",
    "        \n",
    "        self.cwr = sp.lil_matrix(nw_xy / (-nw_xy + nw + nw.T)) #nw.reshape(n,1).T))\n",
    "        \n",
    "        if norm != 1.0:\n",
    "            self.cwr = self.cwr / norm\n",
    "            \n",
    "        coo_matrix = self.cwr.tocoo(copy=False)\n",
    "        df = pd.DataFrame({\n",
    "            'x_id': coo_matrix.row,\n",
    "            'y_id': coo_matrix.col,\n",
    "            'cwr': coo_matrix.data\n",
    "        }).sort_values(['x_id', 'y_id']).reset_index(drop=True)\n",
    "        \n",
    "        df = df.assign(\n",
    "            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "            y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    "        )\n",
    "        \n",
    "        df_nw_x = pd.DataFrame(self.nw_x, columns=['nw'])\n",
    "        \n",
    "        df = df.merge(df_nw_x, left_on='x_id', right_index=True, how='inner').rename(columns={'nw': 'nw_x'})\n",
    "        df = df.merge(df_nw_x, left_on='y_id', right_index=True, how='inner').rename(columns={'nw': 'nw_y'})\n",
    "        \n",
    "        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'cwr']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def cooccurence2(self, direction_sensitive=False, normalize='size', zero_diagonal=True):\n",
    "        n = self.cwr.shape[0]\n",
    "        df = pd.DataFrame([(\n",
    "                i,\n",
    "                j,\n",
    "                self.id2token[i],\n",
    "                self.id2token[j],\n",
    "                self.nw_xy[i,j],\n",
    "                self.nw_x[i],\n",
    "                self.nw_x[j],\n",
    "                self.cwr[i,j]\n",
    "            ) for i,j in itertools.product(range(0,n), repeat=2) if self.cwr[i,j] > 0 ], columns=['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y', 'cwr'])\n",
    "        \n",
    "        return df    \n",
    "    \n",
    "    def cooccurence(self, direction_sensitive=False, normalize='size', zero_diagonal=True):\n",
    "        '''Return computed co-occurrence values'''\n",
    "        \n",
    "        matrix = self.nw_xy\n",
    "        \n",
    "        if not direction_sensitive:\n",
    "            matrix += matrix.T\n",
    "            matrix[np.tril_indices(matrix.shape[0])] = 0\n",
    "        else:\n",
    "            if zero_diagonal:\n",
    "                matrix.fill_diagonal(0)\n",
    "                \n",
    "        coo_matrix = matrix.tocoo(copy=False)\n",
    "        \n",
    "        df_nw_x = pd.DataFrame(self.nw_x, columns=['nw'])\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'x_id': coo_matrix.row,\n",
    "            'y_id': coo_matrix.col,\n",
    "            'nw_xy': coo_matrix.data\n",
    "        })[['x_id', 'y_id', 'nw_xy']].sort_values(['x_id', 'y_id']).reset_index(drop=True)\n",
    "        \n",
    "        df = df.assign(\n",
    "            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "            y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    "        )\n",
    "        \n",
    "        df = df.merge(df_nw_x, left_on='x_id', right_index=True, how='inner').rename(columns={'nw': 'nw_x'})\n",
    "        df = df.merge(df_nw_x, left_on='y_id', right_index=True, how='inner').rename(columns={'nw': 'nw_y'})\n",
    "        \n",
    "        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y']]\n",
    "        \n",
    "        norm = 1.0\n",
    "        if normalize == 'size':\n",
    "            norm = self.term_count\n",
    "        elif normalize == 'max':\n",
    "            norm = np.max(coo_matrix)\n",
    "        elif normalize is None:\n",
    "            logger.warning('No normalize method specified. Using absolute counts...')\n",
    "            pass # return as as is...\"\n",
    "        else:\n",
    "            assert False, 'Unknown normalize specifier'\n",
    "\n",
    "        #logger.info('Normalizing for document corpus size %s.', norm)\n",
    "\n",
    "        df_nw_xy = df.assign(cwr=((df.nw_xy / (df.nw_x + df.nw_y - df.nw_xy)) / norm))\n",
    "\n",
    "        df_nw_xy.loc[df_nw_xy.cwr < 0.0, 'cwr'] = 0\n",
    "        df_nw_xy.cwr.fillna(0.0, inplace=True)\n",
    "        \n",
    "        return df_nw_xy[df_nw_xy.cwr > 0]\n",
    "\n",
    "def test_burgess_litmus_test():\n",
    "    terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "    answer = {\n",
    "     'barn':  {'.': 4,  'barn': 0,  'fell': 5,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    "     'fell':  {'.': 5,  'barn': 0,  'fell': 0,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    "     'horse': {'.': 0,  'barn': 2,  'fell': 1,  'horse': 0,  'past': 4,  'raced': 5,  'the': 3},\n",
    "     'past':  {'.': 2,  'barn': 4,  'fell': 3,  'horse': 0,  'past': 0,  'raced': 0,  'the': 5},\n",
    "     'raced': {'.': 1,  'barn': 3,  'fell': 2,  'horse': 0,  'past': 5,  'raced': 0,  'the': 4},\n",
    "     'the':   {'.': 3,  'barn': 6,  'fell': 4,  'horse': 5,  'past': 3,  'raced': 4,  'the': 2}\n",
    "    }\n",
    "    df_answer = pd.DataFrame(answer).astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n",
    "    #display(df_answer)\n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n",
    "    vectorizer.fit([terms], size=5, distance_metric=0)\n",
    "    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n",
    "    assert df_imp.equals(df_answer), \"Test failed\"\n",
    "    #df_imp == df_answer\n",
    "\n",
    "    # Example in Chen, Lu:\n",
    "    terms = 'The basic concept of the word association'.lower().split()\n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer().fit([terms], size=5, distance_metric=0)\n",
    "    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'basic', 'concept', 'of', 'word', 'association']].sort_index()\n",
    "    df_answer = pd.DataFrame({\n",
    "        'the': [2, 5, 4, 3, 6, 4],\n",
    "        'basic': [3, 0, 5, 4, 2, 1],\n",
    "        'concept': [4, 0, 0, 5, 3, 2], \n",
    "        'of': [5, 0, 0, 0, 4, 3],\n",
    "        'word': [0, 0, 0, 0, 0, 5],\n",
    "        'association': [0, 0, 0, 0, 0, 0]\n",
    "        },\n",
    "        index=['the', 'basic', 'concept', 'of', 'word', 'association'],\n",
    "        dtype=np.int32\n",
    "    ).sort_index()[['the', 'basic', 'concept', 'of', 'word', 'association']]\n",
    "    assert df_imp.equals(df_answer), \"Test failed\"\n",
    "    print('Test run OK')\n",
    "    \n",
    "    \n",
    "    \n",
    "test_burgess_litmus_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Compute Using Prepared Tokenized Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ada1b6926d4aa4a2229d42b0d944ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Corpus', layout=Layout(width='400px'), optiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import os, glob\n",
    "import text_corpus\n",
    "import domain_logic_tCoIR as domain_logic\n",
    "import time\n",
    "\n",
    "class PreparedCorpusUserInterface():\n",
    "    \n",
    "    def __init__(self, data_folder):\n",
    "        self.data_folder = data_folder\n",
    "        \n",
    "    def display(self, compute_handler):\n",
    "        \n",
    "        def on_button_clicked(b):\n",
    "            \n",
    "            if self.filepath.value is None:\n",
    "                return\n",
    "            \n",
    "            with self.out:\n",
    "                self.button.disabled = True\n",
    "                compute_handler(\n",
    "                    self.filepath.value,\n",
    "                    window_size=self.window_size.value,\n",
    "                    distance_metric=self.distance_metric.value,\n",
    "                    direction_sensitive=False, # self.direction_sensitive.value,\n",
    "                    method=self.method.value\n",
    "                )\n",
    "                self.button.disabled = False\n",
    "\n",
    "        corpus_files = sorted(glob.glob(os.path.join(self.data_folder, '*.tokenized.zip')))\n",
    "        distance_metric_options = [\n",
    "            ('linear', 0),\n",
    "            ('inverse', 1),\n",
    "            ('constant', 2)\n",
    "        ]\n",
    "        \n",
    "        self.filepath            = widgets.Dropdown(description='Corpus', options=corpus_files, value=None, layout=widgets.Layout(width='400px'))\n",
    "        self.window_size         = widgets.IntSlider(description='Window', min=2, max=40, value=5, layout=widgets.Layout(width='250px'))\n",
    "        self.method              = widgets.Dropdown(description='Method', options=['HAL', 'Glove'], value='HAL', layout=widgets.Layout(width='200px'))\n",
    "        self.button              = widgets.Button(description='Compute', button_style='Success', layout=widgets.Layout(width='115px',background_color='blue'))\n",
    "        self.out                 = widgets.Output()\n",
    "        \n",
    "        self.distance_metric     = widgets.Dropdown(description='Dist.f.', options=distance_metric_options, value=2, layout=widgets.Layout(width='200px'))\n",
    "        #self.direction_sensitive = widgets.ToggleButton(description='L/R', value=False, layout=widgets.Layout(width='115px',background_color='blue'))\n",
    "        #self.zero_diagonal       = widgets.ToggleButton(description='Zero Diag', value=False, layout=widgets.Layout(width='115px',background_color='blue'))\n",
    "        \n",
    "        self.button.on_click(on_button_clicked)\n",
    "        \n",
    "        return widgets.VBox([\n",
    "            widgets.HBox([\n",
    "                widgets.VBox([\n",
    "                    self.filepath,\n",
    "                    self.method\n",
    "                ]),\n",
    "                widgets.VBox([\n",
    "                    self.window_size,\n",
    "                    self.distance_metric\n",
    "                ]),\n",
    "                widgets.VBox([\n",
    "                    #self.direction_sensitive,\n",
    "                    self.button\n",
    "                ])\n",
    "            ]),\n",
    "            self.out])\n",
    "    \n",
    "import re\n",
    "def source_corpus_filename(tokenized_corpus_name):\n",
    "    try:\n",
    "        m = re.match('(.*\\.txt)_preprocessed.*', tokenized_corpus_name)\n",
    "        return m.groups()[0] + '.zip'\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def do_some_stuff(\n",
    "    filepath,\n",
    "    window_size=5,\n",
    "    distance_metric=0,\n",
    "    direction_sensitive=False,\n",
    "    normalize='size',\n",
    "    zero_diagonal=True,\n",
    "    method='HAL'\n",
    "):\n",
    "\n",
    "    corpus = text_corpus.SimplePreparedTextCorpus(filepath)\n",
    "    doc_terms = [ [ t for t in terms if len(t) > 2] for terms in corpus.get_texts() ]\n",
    "    \n",
    "    common_token2id = build_vocab(doc_terms)\n",
    "    \n",
    "    source_name = source_corpus_filename(filepath)\n",
    "    document_index = domain_logic.compile_unesco_corpus_index(corpus)\n",
    "    source_index = domain_logic.load_corpus_index(source_name).set_index('local_number')\n",
    "    #threshold = 0.005\n",
    "    \n",
    "    dfs = []\n",
    "    # FIXM#:Loop over year instead, and filter out docs for year\n",
    "    for i in range(0, len(doc_terms)):\n",
    "        \n",
    "        doc          = doc_terms[i] \n",
    "        local_number = document_index.iloc[i].local_number\n",
    "        year         = source_index.loc[local_number].year\n",
    "        \n",
    "        logger.info('Year %s...', year)\n",
    "        \n",
    "        if method == \"HAL\":\n",
    "            \n",
    "            vectorizer = HyperspaceAnalogueToLanguageVectorizer(token2id=common_token2id)\\\n",
    "                .fit([doc], size=window_size, distance_metric=distance_metric)\n",
    "        \n",
    "            df = vectorizer.cooccurence(direction_sensitive=direction_sensitive, normalize=normalize, zero_diagonal=zero_diagonal)\n",
    "        else:\n",
    "            \n",
    "            vectorizer = GloveVectorizer(token2id=common_token2id)\\\n",
    "                .fit([doc], size=window_size)\n",
    "            \n",
    "            df = vectorizer.cooccurence(normalize=normalize, zero_diagonal=zero_diagonal)\n",
    "            \n",
    "        df['year'] = year\n",
    "        #df = df[df.cwr >= threshhold]\n",
    "        \n",
    "        dfs.append(df[['year', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y', 'cwr']])\n",
    "\n",
    "        #if i == 5: break\n",
    "            \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    df['cwr'] = df.cwr / np.max(df.cwr, axis=0)\n",
    "    #display(df.sort_values('cwr', ascending=False).head(100))\n",
    "\n",
    "    result_filename = 'result_co_occurrence_{}.txt'.format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    \n",
    "    df.to_csv(result_filename, sep='\\t')\n",
    "    print('Result saved to file {}'.format(result_filename))\n",
    "    \n",
    "    print('Now you are ready to do some serious stuff!')\n",
    "    #return doc_terms\n",
    "    \n",
    "display(PreparedCorpusUserInterface(domain_logic.DATA_FOLDER).display(do_some_stuff))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip', '../../data/unesco_conference_material_corpus.txt.zip', '../../data/term_substitutions.txt', '../../data/benedict-xvi_curated_20190326.txt_preprocessed_en__disable(parser,textcat)_.pkl.bz2', '../../data/unesco_dataset_index.xlsx', '../../data/unesco_conference_material_corpus.txt_preprocessed.zip', '../../data/benedict-xvi_curated_20190326.txt_preprocessed_201903011731.tokenized.zip', '../../data/unesco_courier_corpus.txt_preprocessed_nnp_frq10_ng2.tokenized.zip', '../../data/treaty_text_corpora_20181206.txt_preprocessed.zip', '../../data/unesco_courier_corpus.txt.zip', '../../data/unesco_courier_corpus.txt_preprocessed_nnpadj_frq10_ng2.tokenized.zip', '../../data/tagset.csv', '../../data/benedict-xvi_curated_20190326.txt_preprocessed_en__disable(parser,ner,textcat)_.pkl.bz2', '../../data/benedict-xvi_curated_20190326.txt_preprocessed_en__disable(parser,ner,textcat)_.bin.bz2', '../../data/unesco_courier_corpus_stats.txt', '../../data/unesco_conference_material_corpus_stats.txt', '../../data/unesco_courier_corpus.txt_preprocessed_en__disable(parser,ner,textcat)_.bin.bz2', '../../data/benedict-xvi_curated_20190326.txt.zip', '../../data/unesco_courier_corpus.txt_preprocessed.zip']\n"
     ]
    }
   ],
   "source": [
    "domain_logic.DATA_FOLDER\n",
    "import glob\n",
    "print(glob.glob(domain_logic.DATA_FOLDER + \"/*.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PLAYGROUND </span> Ignore  <span style='float: right; color: red'>SKIP</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = [ current_corpus()[0] ]\n",
    "#terms = [ list(doc) for doc in textacy_utility.extract_corpus_terms(corpus, extract_args) ]\n",
    "terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n",
    "vectorizer.fit(terms, size=5)\n",
    "print(vectorizer)\n",
    "matrix = vectorizer.p_ij\n",
    "\n",
    "matrix += matrix.T\n",
    "matrix[np.tril_indices(matrix.shape[0])] = 0\n",
    "coo_matrix = matrix.tocoo(copy=False)\n",
    "list(coo_matrix.todense())\n",
    "\n",
    "df_p_i = pd.DataFrame(self.p_i, columns=['p_i_count'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x_id': coo_matrix.row,\n",
    "    'y_id': coo_matrix.col,\n",
    "    'p_xy': coo_matrix.data\n",
    "})[['x_id', 'y_id', 'p_xy']]\\\n",
    ".sort_values(['x_id', 'y_id'])\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "df = df.assign(\n",
    "    x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "    y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    ")\n",
    "\n",
    "df = df.merge(df_p_i, left_on='x_id', right_index=True, how='inner').rename(columns={'p_i_count': 'p_x'})\n",
    "df = df.merge(df_p_i, left_on='y_id', right_index=True, how='inner').rename(columns={'p_i_count': 'p_y'})\n",
    "\n",
    "df = df[['x_id', 'y_id', 'x_term', 'y_term', 'p_xy', 'p_x', 'p_y']]\n",
    "\n",
    "if normalize_by_term_count:\n",
    "    df['p_xy'] /= self.term_count\n",
    "    df['p_x']  /= self.term_count\n",
    "    df['p_y']  /= self.term_count\n",
    "\n",
    "df = df.assign(score=df.p_xy / (df.p_x + df.p_y - df.p_xy))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "   \n",
    "def compute_co_occurrence(corpus, gui, documents, document_filters, terms_filter, window_size, group_by_columns, weighting, tick):\n",
    "    \n",
    "    # FIXME: Split based on group_by_filters\n",
    "    docs = list(gui_utility.get_documents_by_field_filters(corpus, documents, document_filters))\n",
    "    print(len(docs))\n",
    "    gui.progress.max = len(docs)\n",
    "    \n",
    "    #def terms():\n",
    "    #    for doc in doc:\n",
    "    #        gui.progress.value += 1\n",
    "    #        yield textacy_utility.extract_corpus_terms(docs, terms_filter)\n",
    "            \n",
    "    #terms = [ list(doc) for doc in textacy_utility.extract_corpus_terms(docs, terms_filter) ]\n",
    "    \n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer(tick=tick)\n",
    "    vectorizer.fit(terms, size=window_size, weighing=weighting)\n",
    "    logger.warning('Computation is based on (P_ij + P_ij.T) by default (before/after weights are summed up) )')\n",
    "    df = vectorizer.cooccurence()\n",
    "    df.sort_values('score', ascending=False)\n",
    "    return df.head(500)\n",
    "    \n",
    "def display_co_occurrences(gui, df):\n",
    "    display(df)\n",
    "\n",
    "def word_co_occurrence_gui(documents, corpus, compute_callback, display_callback, filter_options, group_by_options):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    weighting_options = { 'Linear': 0, 'Reciprocal': 1, 'Constant': 2 }\n",
    "    normalize_options = { '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    default_include_pos = ['NOUN', 'PROPN']\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100, include_pos=default_include_pos) ]\n",
    "\n",
    "    output_type_options = [ ( 'List', 'table' ), ( 'Rank', 'rank' ) ]\n",
    "    ngrams_options = { '-': None, '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    \n",
    "    document_filters = gui_utility.generate_field_filters(documents, filter_options)\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=lw('98%')),\n",
    "        document_filters=document_filters,\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=lw('200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=lw('200px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options=weighting_options, value=0, layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=default_include_pos, rows=7, layout=lw('150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=lw('200px')),\n",
    "        group_by_columns=widgets.Dropdown(description='Group by', value=group_by_options[0][1], options=group_by_options, layout=lw('200px')),\n",
    "        #output_type=widgets.Dropdown(description='Output', value='rank', options=output_type_options, layout=lw('200px')),\n",
    "        window_size=widgets.IntSlider(description='Window', min=5, max=20, value=5, layout=lw('200px')),\n",
    "        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=1, layout=lw('200px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    def tick():\n",
    "        gui.progress.value += 1\n",
    "        \n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.weighting,\n",
    "                gui.group_by_columns,\n",
    "                gui.min_freq,\n",
    "                gui.window_size\n",
    "                #gui.output_type,\n",
    "            ]),\n",
    "            widgets.VBox([ x['widget'] for x in gui.document_filters]),\n",
    "            gui.include_pos,\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                gui.compute,\n",
    "            ], layout=widgets.Layout(align_items='flex-end')),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def pos_change_handler(*args):\n",
    "        with gui.output:\n",
    "            gui.compute.disabled = True\n",
    "            selected = set(gui.stop_words.value)\n",
    "            frequent_words = [\n",
    "                x[0] for x in textacy_utility.get_most_frequent_words(\n",
    "                    corpus,\n",
    "                    100,\n",
    "                    normalize=gui.normalize.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    weighting=gui.weighting.value\n",
    "                )\n",
    "            ]\n",
    "            gui.stop_words.options = frequent_words\n",
    "            selected = selected & set(gui.stop_words.options)\n",
    "            gui.stop_words.value = list(selected)\n",
    "            gui.compute.disabled = False\n",
    "        \n",
    "    gui.include_pos.observe(pos_change_handler, 'value')    \n",
    "    gui.weighting.observe(pos_change_handler, 'value')    \n",
    "    \n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            try:\n",
    "                gui.compute.disabled = True\n",
    "                terms_filter = dict(\n",
    "                    args=dict(\n",
    "                        ngrams=gui.ngrams.value,\n",
    "                        named_entities=None,\n",
    "                        normalize=gui.normalize.value,\n",
    "                        as_strings=True\n",
    "                    ),\n",
    "                    kwargs=dict(\n",
    "                        min_freq=gui.min_freq.value,\n",
    "                        include_pos=gui.include_pos.value,\n",
    "                        filter_stops=True,\n",
    "                        filter_punct=True\n",
    "                    ),\n",
    "                    extra_stop_words=set(gui.stop_words.value),\n",
    "                    substitutions=None\n",
    "                )\n",
    "                df = compute_callback(\n",
    "                    corpus=corpus,\n",
    "                    gui=gui,\n",
    "                    documents=documents,\n",
    "                    document_filters=[ (x['field'], x['widget'].value) for x in gui.document_filters],\n",
    "                    terms_filter=terms_filter,\n",
    "                    window_size=gui.window_size.value,\n",
    "                    group_by_columns=gui.group_by_columns.value,\n",
    "                    weighting=gui.weighting.value,\n",
    "                    tick=tick\n",
    "                )\n",
    "                display_callback(gui, df)\n",
    "            finally:\n",
    "                gui.compute.disabled = False\n",
    "                gui.progress.value = 0\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "                \n",
    "try:\n",
    "    document_index = domain_logic.compile_documents(current_corpus())\n",
    "    word_co_occurrence_gui(\n",
    "        document_index,\n",
    "        current_corpus(),\n",
    "        compute_callback=compute_co_occurrence,\n",
    "        display_callback=display_co_occurrences,\n",
    "        filter_options=domain_logic.DOCUMENT_FILTERS,\n",
    "        group_by_options=domain_logic.GROUP_BY_OPTIONS\n",
    "    )\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
