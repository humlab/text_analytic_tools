{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tCoIR - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import textacy.keyterms\n",
    "import gui_utility\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER = '../../data'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n",
    "current_document_index = lambda: current_corpus_container().document_index\n",
    "\n",
    "import domain_logic_vatican as domain_logic\n",
    "\n",
    "extract_args = dict(\n",
    "    args=dict(\n",
    "        ngrams=[1],\n",
    "        named_entities=False,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        min_freq=1,\n",
    "        include_pos=['NOUN'],\n",
    "        filter_stops=True,\n",
    "        filter_punct=True\n",
    "    ),\n",
    "    extra_stop_words=None,\n",
    "    substitutions=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7f81cd6ff64255bbc8d025ba875961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(Dropdown(description='C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, container=container)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> HAL Co-Windows Ratio (CWR)<span style='float: right; color: red'>MANDATORY</span>\n",
    "\n",
    "\\begin{aligned}\n",
    "nw(x) &= \\text{number of sliding windows that contains term $x$} \\\\\n",
    "nw(x, y) &= \\text{number of sliding windows that contains $x$ and $y$} \\\\\n",
    "\\\\\n",
    "f(x, y) &= \\text{normalized version of nw(x, y)} \\\\\n",
    "CWR(x, y) &= \\frac{nw(x, y)}{nw(x) + nw(y) - nw(x, y)}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Term co-occurrence frequencies is calculated in accordance with Hyperspace Analogue to Language (Lund; Burgess, 1996) vector-space model. The computation is specified in detail in section 3.1 in (Chen; Lu, 2011).\n",
    "\n",
    "- Chen Z.; Lu Y., \"A Word Co-occurrence Matrix Based Method for Relevance Feedback\"\n",
    "- Lund, K.; Burgess, C. & Atchley, R. A. (1995). \"Semantic and associative priming in high-dimensional semantic space\".[Link](https://books.google.de/books?id=CSU_Mj07G7UC).\n",
    "- Lund, K.; Burgess, C. (1996). \"Producing high-dimensional semantic spaces from lexical co-occurrence\". doi:10.3758/bf03204766 [Link](https://dx.doi.org/10.3758%2Fbf03204766).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-25 15:35:28,994 : INFO : Builiding vocabulary...\n",
      "2019-02-25 15:35:28,996 : INFO : Vocabulary of size 7 built from 8 terms.\n",
      "2019-02-25 15:35:29,004 : INFO : Builiding vocabulary...\n",
      "2019-02-25 15:35:29,006 : INFO : Vocabulary of size 6 built from 7 terms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test run OK\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "import scipy.sparse as sp\n",
    "import itertools\n",
    "\n",
    "class HyperspaceAnalogueToLanguageVectorizer():\n",
    "    \n",
    "    def __init__(self, corpus=None, token2id=None, tick=utility.noop):\n",
    "        \"\"\"\n",
    "        Build vocabulary and create nw_xy term-term matrix and nw_x term global occurence vector\n",
    "        \n",
    "        Parameter:\n",
    "            corpus Iterable[Iterable[str]]\n",
    "\n",
    "        \"\"\"\n",
    "        self.token2id = token2id\n",
    "        self.corpus = corpus\n",
    "        self.term_count = 0\n",
    "        \n",
    "        if corpus is not None and token2id is None:\n",
    "            self._build_vocabulary(corpus)\n",
    "        \n",
    "        self.nw_xy = None\n",
    "        self.nw_x = None\n",
    "        self._id2token = None\n",
    "        self.tick = tick\n",
    "        \n",
    "    def _build_vocabulary(self, corpus):\n",
    "        ''' Iterates corpus and add distict terms to vocabulary '''\n",
    "        logger.info('Builiding vocabulary...')\n",
    "        token2id = collections.defaultdict()\n",
    "        token2id.default_factory = token2id.__len__\n",
    "        term_count = 0\n",
    "        for doc in corpus:\n",
    "            for term in doc:\n",
    "                token2id[term]\n",
    "                term_count += 1\n",
    "        self.token2id = token2id\n",
    "        self.term_count = term_count\n",
    "        logger.info('Vocabulary of size {} built from {} terms.'.format(len(token2id), term_count))\n",
    "    \n",
    "    @property\n",
    "    def id2token(self):\n",
    "        if self._id2token is None:\n",
    "            if self.token2id is not None:\n",
    "                self._id2token = { v:k for k,v in self.token2id.items() }\n",
    "        return self._id2token\n",
    "    \n",
    "    def sliding_window(self, seq, n):\n",
    "        it = itertools.chain(iter(seq), [None] * n)\n",
    "        memory = tuple(itertools.islice(it, n+1))\n",
    "        if len(memory) == n+1:\n",
    "            yield memory\n",
    "        for x in it:\n",
    "            memory = memory[1:] + (x,)\n",
    "            yield memory\n",
    "        \n",
    "    def fit(self, corpus=None, size=2, weighing=0, zero_out_diag=False):\n",
    "        \n",
    "        '''Trains HAL for a document. Note that sentence borders (for now) are ignored'''\n",
    "        \n",
    "        if corpus is not None:\n",
    "            self.corpus = corpus\n",
    "            self._build_vocabulary(corpus)\n",
    "            \n",
    "        assert self.token2id is not None, \"Fit with no vocabulary!\"\n",
    "        assert self.corpus is not None, \"Fit with no corpus!\"\n",
    "\n",
    "        nw_xy = sp.lil_matrix ((len(self.token2id), len(self.token2id)), dtype=np.int32)\n",
    "        nw_x = np.zeros(len(self.token2id), dtype=np.int32)\n",
    "        \n",
    "        for terms in corpus:\n",
    "            \n",
    "            id_terms = ( self.token2id[size] for size in terms)\n",
    "            \n",
    "            self.tick()\n",
    "            \n",
    "            for win in self.sliding_window(id_terms, size):\n",
    "                \n",
    "                #logger.info([ self.id2token[x] if x is not None else None for x in win])\n",
    "                \n",
    "                if win[0] is None:\n",
    "                    continue\n",
    "                    \n",
    "                for x in win:\n",
    "                    if x is not None:\n",
    "                        nw_x[x] += 1\n",
    "\n",
    "                for i in range(1, size+1):\n",
    "\n",
    "                    if win[i] is None:\n",
    "                        continue\n",
    "                        \n",
    "                    if zero_out_diag:\n",
    "                        if win[0] == win[i]:\n",
    "                            continue\n",
    "                        \n",
    "                    d = i # abs(n - i)\n",
    "                    if weighing == 0: #  linear i.e. adjacent equals window size, then decreasing by one\n",
    "                        w = size - d + 1\n",
    "                    elif weighing == 1: # f(d) = 1 / d\n",
    "                        w = 1.0 / d\n",
    "                    elif weighing == 2: # Constant value of 1\n",
    "                        w = 1\n",
    "\n",
    "                    #print('*', i, self.id2token[win[0]], self.id2token[win[i]], w, [ self.id2token[x] if x is not None else None for x in win])\n",
    "                    nw_xy[win[0], win[i]] += w\n",
    "                    \n",
    "        self.nw_x = nw_x\n",
    "        self.nw_xy = nw_xy\n",
    "        #self.f_xy = nw_xy / np.max(nw_xy)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def to_df(self):\n",
    "        columns = [ self.id2token[i] for i in range(0,len(self.token2id))]\n",
    "        return pd.DataFrame(\n",
    "            data=self.nw_xy.todense(),\n",
    "            index=list(columns),\n",
    "            columns=list(columns),\n",
    "            dtype=np.float64\n",
    "        ).T\n",
    "    \n",
    "    def cwr(self, direction_sensitive=False, normalize='size'):\n",
    "\n",
    "        n = self.nw_x.shape[0]\n",
    "        \n",
    "        nw = self.nw_x.reshape(n,1)\n",
    "        nw_xy = self.nw_xy\n",
    "        \n",
    "        norm = 1.0\n",
    "        if normalize == 'size':\n",
    "            norm = float(self.term_count)\n",
    "        elif norm == 'max':\n",
    "            norm = float(np.max(nw_xy))\n",
    "        \n",
    "        #nw.resize(nw.shape[0], 1)\n",
    "        \n",
    "        self.cwr = sp.lil_matrix(nw_xy / (-nw_xy + nw + nw.T)) #nw.reshape(n,1).T))\n",
    "        \n",
    "        if norm != 1.0:\n",
    "            self.cwr = self.cwr / norm\n",
    "            \n",
    "        coo_matrix = self.cwr.tocoo(copy=False)\n",
    "        df = pd.DataFrame({\n",
    "            'x_id': coo_matrix.row,\n",
    "            'y_id': coo_matrix.col,\n",
    "            'cwr': coo_matrix.data\n",
    "        }).sort_values(['x_id', 'y_id']).reset_index(drop=True)\n",
    "        \n",
    "        df = df.assign(\n",
    "            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "            y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    "        )\n",
    "        df_nw_x = pd.DataFrame(self.nw_x, columns=['nw'])\n",
    "\n",
    "        \n",
    "        df = df.merge(df_nw_x, left_on='x_id', right_index=True, how='inner').rename(columns={'nw': 'nw_x'})\n",
    "        df = df.merge(df_nw_x, left_on='y_id', right_index=True, how='inner').rename(columns={'nw': 'nw_y'})\n",
    "        \n",
    "        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'cwr']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def cooccurence2(self, direction_sensitive=False, normalize='size', zero_diagonal=True):\n",
    "        n = self.cwr.shape[0]\n",
    "        df = pd.DataFrame([(\n",
    "                i,\n",
    "                j,\n",
    "                self.id2token[i],\n",
    "                self.id2token[j],\n",
    "                self.nw_xy[i,j],\n",
    "                self.nw_x[i],\n",
    "                self.nw_x[j],\n",
    "                self.cwr[i,j]\n",
    "            ) for i,j in itertools.product(range(0,n), repeat=2) if self.cwr[i,j] > 0 ], columns=['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y', 'cwr'])\n",
    "        \n",
    "        return df    \n",
    "    \n",
    "    def cooccurence(self, direction_sensitive=False, normalize='size', zero_diagonal=True):\n",
    "        '''Return computed co-occurrence values'''\n",
    "        \n",
    "        matrix = self.nw_xy\n",
    "        \n",
    "        if not direction_sensitive:\n",
    "            matrix += matrix.T\n",
    "            matrix[np.tril_indices(matrix.shape[0])] = 0\n",
    "            coo_matrix = matrix.tocoo(copy=False)\n",
    "        else:\n",
    "            if zero_diagonal:\n",
    "                matrix.fill_diagonal(0)\n",
    "            coo_matrix = matrix.tocoo(copy=False)\n",
    "        \n",
    "        df_nw_x = pd.DataFrame(self.nw_x, columns=['nw'])\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'x_id': coo_matrix.row,\n",
    "            'y_id': coo_matrix.col,\n",
    "            'nw_xy': coo_matrix.data\n",
    "        })[['x_id', 'y_id', 'nw_xy']].sort_values(['x_id', 'y_id']).reset_index(drop=True)\n",
    "        \n",
    "        df = df.assign(\n",
    "            x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "            y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    "        )\n",
    "        \n",
    "        df = df.merge(df_nw_x, left_on='x_id', right_index=True, how='inner').rename(columns={'nw': 'nw_x'})\n",
    "        df = df.merge(df_nw_x, left_on='y_id', right_index=True, how='inner').rename(columns={'nw': 'nw_y'})\n",
    "        \n",
    "        df = df[['x_id', 'y_id', 'x_term', 'y_term', 'nw_xy', 'nw_x', 'nw_y']]\n",
    "        \n",
    "        if normalize == 'size':\n",
    "            df['nw_xy'] /= self.term_count\n",
    "            df['nw_x']  /= self.term_count\n",
    "            df['nw_y']  /= self.term_count\n",
    "        elif normalize == 'max':\n",
    "            df['nw_xy'] /= df.nw_xy.max()\n",
    "            df['nw_x']  /= df.nw_x.max()\n",
    "            df['nw_y']  /= df.nw_x.max()\n",
    "        \n",
    "        df = df.assign(cwr=df.nw_xy / (df.nw_x + df.nw_y - df.nw_xy))\n",
    "        \n",
    "        return df\n",
    "\n",
    "def test_burgess_litmus_test():\n",
    "    terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "    answer = {\n",
    "     'barn':  {'.': 4,  'barn': 0,  'fell': 5,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    "     'fell':  {'.': 5,  'barn': 0,  'fell': 0,  'horse': 0,  'past': 0,  'raced': 0,  'the': 0},\n",
    "     'horse': {'.': 0,  'barn': 2,  'fell': 1,  'horse': 0,  'past': 4,  'raced': 5,  'the': 3},\n",
    "     'past':  {'.': 2,  'barn': 4,  'fell': 3,  'horse': 0,  'past': 0,  'raced': 0,  'the': 5},\n",
    "     'raced': {'.': 1,  'barn': 3,  'fell': 2,  'horse': 0,  'past': 5,  'raced': 0,  'the': 4},\n",
    "     'the':   {'.': 3,  'barn': 6,  'fell': 4,  'horse': 5,  'past': 3,  'raced': 4,  'the': 2}\n",
    "    }\n",
    "    df_answer = pd.DataFrame(answer).astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n",
    "    #display(df_answer)\n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n",
    "    vectorizer.fit([terms], size=5)\n",
    "    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'horse', 'raced', 'past', 'barn', 'fell']].sort_index()\n",
    "    assert df_imp.equals(df_answer), \"Test failed\"\n",
    "    #df_imp == df_answer\n",
    "\n",
    "    # Example in Chen, Lu:\n",
    "    terms = 'The basic concept of the word association'.lower().split()\n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer().fit([terms], size=5)\n",
    "    df_imp = vectorizer.to_df().astype(np.int32)[['the', 'basic', 'concept', 'of', 'word', 'association']].sort_index()\n",
    "    df_answer = pd.DataFrame({\n",
    "        'the': [2, 5, 4, 3, 6, 4],\n",
    "        'basic': [3, 0, 5, 4, 2, 1],\n",
    "        'concept': [4, 0, 0, 5, 3, 2], \n",
    "        'of': [5, 0, 0, 0, 4, 3],\n",
    "        'word': [0, 0, 0, 0, 0, 5],\n",
    "        'association': [0, 0, 0, 0, 0, 0]\n",
    "        },\n",
    "        index=['the', 'basic', 'concept', 'of', 'word', 'association'],\n",
    "        dtype=np.int32\n",
    "    ).sort_index()[['the', 'basic', 'concept', 'of', 'word', 'association']]\n",
    "    assert df_imp.equals(df_answer), \"Test failed\"\n",
    "    print('Test run OK')\n",
    "    \n",
    "    \n",
    "    \n",
    "test_burgess_litmus_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-25 15:36:29,149 : INFO : Builiding vocabulary...\n",
      "2019-02-25 15:36:29,151 : INFO : Vocabulary of size 6 built from 7 terms.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fcd1aaac0e43d380299b4a847638d8",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "normalize = 'size'\n",
    "\n",
    "terms = 'The basic concept of the word association'.lower().split()\n",
    "vectorizer = HyperspaceAnalogueToLanguageVectorizer().fit([terms], size=5)\n",
    "id2token = vectorizer.id2token\n",
    "df = vectorizer.cwr(normalize=normalize)\n",
    "\n",
    "df2 = vectorizer.cooccurence(direction_sensitive=True, normalize=normalize, zero_diagonal=False)\n",
    "\n",
    "df.merge(df2[['x_id', 'y_id', 'cwr']], left_on=['x_id', 'y_id'], right_on=['x_id', 'y_id'], how='outer')\n",
    "#df_imp = vectorizer.to_df().astype(np.int32)[vectorizer.id2token.values()] #.sort_index()\n",
    "#a = { x: list(df_imp[x]) for x in df_imp.columns }; a['index'] = list(df_imp.index)\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (2, 5),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 5),\n",
       " (5, 0),\n",
       " (5, 1),\n",
       " (5, 2),\n",
       " (5, 3),\n",
       " (5, 4),\n",
       " (5, 5)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "list(itertools.product(range(0,6),repeat=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    [ 5, 1 , 3],\n",
    "    [ 1, 1 , 2],\n",
    "    [ 1, 2 , 1]\n",
    "])\n",
    "\n",
    "b = np.array(\n",
    "    [1, 2, 3]\n",
    ")\n",
    "b.resize(3,1)\n",
    "a\n",
    "b\n",
    "a - (b.T)\n",
    "\n",
    "a - b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/sparse/base.py:597: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(self.todense(), other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.2       , 1.66666667, 0.8       , 0.42857143, 1.        ,\n",
       "         0.5       ],\n",
       "        [0.6       , 0.        ,        inf, 2.        , 0.33333333,\n",
       "         0.14285714],\n",
       "        [0.8       , 0.        , 0.        , 2.5       , 0.5       ,\n",
       "         0.28571429],\n",
       "        [1.        , 0.        , 0.        , 0.        , 0.66666667,\n",
       "         0.42857143],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.71428571],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.nw_x.resize(vectorizer.nw_x.shape[0], 1)\n",
    "vectorizer.nw_xy / (-vectorizer.nw_xy + vectorizer.nw_x + vectorizer.nw_x.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-25 08:56:18,036 : INFO : Builiding vocabulary...\n",
      "2019-02-25 08:56:18,038 : INFO : Vocabulary of size 15 built from 29 terms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[matrix([[ 0., 10.,  8.,  0.,  0.,  5.,  4.,  0.,  0.,  3.,  0.,  0.,  0.,\n",
       "           0.,  0.]]),\n",
       " matrix([[ 0.,  0., 12.,  5.,  4.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.]]),\n",
       " matrix([[0., 0., 0., 3., 7., 5., 4., 5., 5., 0., 0., 0., 5., 9., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 5., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[ 0.,  0.,  0.,  0.,  0.,  5., 10.,  4.,  2.,  0.,  4.,  5.,  0.,\n",
       "           0.,  0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 5., 0., 0., 4., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 5., 3., 5., 5., 4., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 7., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " matrix([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = [ current_corpus()[0] ]\n",
    "#terms = [ list(doc) for doc in textacy_utility.extract_corpus_terms(corpus, extract_args) ]\n",
    "terms = 'The Horse Raced Past The Barn Fell .'.lower().split()\n",
    "vectorizer = HyperspaceAnalogueToLanguageVectorizer()\n",
    "vectorizer.fit(terms, size=5)\n",
    "matrix = vectorizer.p_ij\n",
    "\n",
    "matrix += matrix.T\n",
    "matrix[np.tril_indices(matrix.shape[0])] = 0\n",
    "coo_matrix = matrix.tocoo(copy=False)\n",
    "list(coo_matrix.todense())\n",
    "\n",
    "df_p_i = pd.DataFrame(self.p_i, columns=['p_i_count'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x_id': coo_matrix.row,\n",
    "    'y_id': coo_matrix.col,\n",
    "    'p_xy': coo_matrix.data\n",
    "})[['x_id', 'y_id', 'p_xy']]\\\n",
    ".sort_values(['x_id', 'y_id'])\\\n",
    ".reset_index(drop=True)\n",
    "\n",
    "df = df.assign(\n",
    "    x_term=df.x_id.apply(lambda x: self.id2token[x]),\n",
    "    y_term=df.y_id.apply(lambda x: self.id2token[x])\n",
    ")\n",
    "\n",
    "df = df.merge(df_p_i, left_on='x_id', right_index=True, how='inner').rename(columns={'p_i_count': 'p_x'})\n",
    "df = df.merge(df_p_i, left_on='y_id', right_index=True, how='inner').rename(columns={'p_i_count': 'p_y'})\n",
    "\n",
    "df = df[['x_id', 'y_id', 'x_term', 'y_term', 'p_xy', 'p_x', 'p_y']]\n",
    "\n",
    "if normalize_by_term_count:\n",
    "    df['p_xy'] /= self.term_count\n",
    "    df['p_x']  /= self.term_count\n",
    "    df['p_y']  /= self.term_count\n",
    "\n",
    "df = df.assign(score=df.p_xy / (df.p_x + df.p_y - df.p_xy))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43995e88c434a16851d30c94ee699b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='98%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "   \n",
    "def compute_co_occurrence(corpus, gui, documents, document_filters, terms_filter, window_size, group_by_columns, weighting, tick):\n",
    "    \n",
    "    # FIXME: Split based on group_by_filters\n",
    "    docs = list(gui_utility.get_documents_by_field_filters(corpus, documents, document_filters))\n",
    "    print(len(docs))\n",
    "    gui.progress.max = len(docs)\n",
    "    \n",
    "    #def terms():\n",
    "    #    for doc in doc:\n",
    "    #        gui.progress.value += 1\n",
    "    #        yield textacy_utility.extract_corpus_terms(docs, terms_filter)\n",
    "            \n",
    "    #terms = [ list(doc) for doc in textacy_utility.extract_corpus_terms(docs, terms_filter) ]\n",
    "    \n",
    "    vectorizer = HyperspaceAnalogueToLanguageVectorizer(tick=tick)\n",
    "    vectorizer.fit(terms, size=window_size, weighing=weighting)\n",
    "    logger.warning('Computation is mased on (P_ij + P_ij.T) by defauöt (before/after weights are sumed up) )')\n",
    "    df = vectorizer.cooccurence()\n",
    "    df.sort_values('score', ascending=False)\n",
    "    return df.head(500)\n",
    "    \n",
    "def display_co_occurrences(gui, df):\n",
    "    display(df)\n",
    "\n",
    "def word_co_occurrence_gui(documents, corpus, compute_callback, display_callback, filter_options, group_by_options):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    weighting_options = { 'Linear': 0, 'Reciprocal': 1, 'Constant': 2 }\n",
    "    normalize_options = { '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    default_include_pos = ['NOUN', 'PROPN']\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100, include_pos=default_include_pos) ]\n",
    "\n",
    "    output_type_options = [ ( 'List', 'table' ), ( 'Rank', 'rank' ) ]\n",
    "    ngrams_options = { '-': None, '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    \n",
    "    document_filters = gui_utility.generate_field_filters(documents, filter_options)\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=lw('98%')),\n",
    "        document_filters=document_filters,\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=lw('200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=lw('200px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options=weighting_options, value=0, layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=default_include_pos, rows=7, layout=lw('150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=lw('200px')),\n",
    "        group_by_columns=widgets.Dropdown(description='Group by', value=group_by_options[0][1], options=group_by_options, layout=lw('200px')),\n",
    "        #output_type=widgets.Dropdown(description='Output', value='rank', options=output_type_options, layout=lw('200px')),\n",
    "        window_size=widgets.IntSlider(description='Window', min=5, max=20, value=5, layout=lw('200px')),\n",
    "        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=1, layout=lw('200px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    def tick():\n",
    "        gui.progress.value += 1\n",
    "        \n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.weighting,\n",
    "                gui.group_by_columns,\n",
    "                gui.min_freq,\n",
    "                gui.window_size\n",
    "                #gui.output_type,\n",
    "            ]),\n",
    "            widgets.VBox([ x['widget'] for x in gui.document_filters]),\n",
    "            gui.include_pos,\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                gui.compute,\n",
    "            ], layout=widgets.Layout(align_items='flex-end')),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def pos_change_handler(*args):\n",
    "        with gui.output:\n",
    "            gui.compute.disabled = True\n",
    "            selected = set(gui.stop_words.value)\n",
    "            frequent_words = [\n",
    "                x[0] for x in textacy_utility.get_most_frequent_words(\n",
    "                    corpus,\n",
    "                    100,\n",
    "                    normalize=gui.normalize.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    weighting=gui.weighting.value\n",
    "                )\n",
    "            ]\n",
    "            gui.stop_words.options = frequent_words\n",
    "            selected = selected & set(gui.stop_words.options)\n",
    "            gui.stop_words.value = list(selected)\n",
    "            gui.compute.disabled = False\n",
    "        \n",
    "    gui.include_pos.observe(pos_change_handler, 'value')    \n",
    "    gui.weighting.observe(pos_change_handler, 'value')    \n",
    "    \n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            try:\n",
    "                gui.compute.disabled = True\n",
    "                terms_filter = dict(\n",
    "                    args=dict(\n",
    "                        ngrams=gui.ngrams.value,\n",
    "                        named_entities=None,\n",
    "                        normalize=gui.normalize.value,\n",
    "                        as_strings=True\n",
    "                    ),\n",
    "                    kwargs=dict(\n",
    "                        min_freq=gui.min_freq.value,\n",
    "                        include_pos=gui.include_pos.value,\n",
    "                        filter_stops=True,\n",
    "                        filter_punct=True\n",
    "                    ),\n",
    "                    extra_stop_words=set(gui.stop_words.value),\n",
    "                    substitutions=None\n",
    "                )\n",
    "                df = compute_callback(\n",
    "                    corpus=corpus,\n",
    "                    gui=gui,\n",
    "                    documents=documents,\n",
    "                    document_filters=[ (x['field'], x['widget'].value) for x in gui.document_filters],\n",
    "                    terms_filter=terms_filter,\n",
    "                    window_size=gui.window_size.value,\n",
    "                    group_by_columns=gui.group_by_columns.value,\n",
    "                    weighting=gui.weighting.value,\n",
    "                    tick=tick\n",
    "                )\n",
    "                display_callback(gui, df)\n",
    "            finally:\n",
    "                gui.compute.disabled = False\n",
    "                gui.progress.value = 0\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "                \n",
    "try:\n",
    "    document_index = domain_logic.compile_documents(current_corpus())\n",
    "    word_co_occurrence_gui(\n",
    "        document_index,\n",
    "        current_corpus(),\n",
    "        compute_callback=compute_co_occurrence,\n",
    "        display_callback=display_co_occurrences,\n",
    "        filter_options=domain_logic.DOCUMENT_FILTERS,\n",
    "        group_by_options=domain_logic.GROUP_BY_OPTIONS\n",
    "    )\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index.query('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-130-b3b046b78708>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-130-b3b046b78708>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cimport cython\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cimport cython\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cpdef make_lower_triangular(double[:,:] A, int k):\n",
    "    \"\"\" Set all the entries of array A that lie above\n",
    "    diagonal k to 0. \"\"\"\n",
    "    cdef int i, j\n",
    "    for i in range(min(A.shape[0], A.shape[0] - k)):\n",
    "        for j in range(max(0, i+k+1), A.shape[1]):\n",
    "            A[i,j] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
