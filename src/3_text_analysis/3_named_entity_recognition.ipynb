{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os #, collections, zipfile\n",
    "#import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import domain_logic_vatican as domain_logic\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER, PATTERN = '../../data',  '*.txt'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set_matplotlib_formats('svg')   \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376f78f48718446a8b44dbcff2b26908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    # FIXME VARYING ASPECTS: document_index = WTI_INDEX for tCoIR\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, document_index=None, container=container, compute_ner=True, domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e25be327b94135a4080be4f6757dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Document', index=1, layout=Layout(width='50%'), options=((…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, document_index):\n",
    "    \n",
    "    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n",
    "    filenames = document_index.filename\n",
    "    document_options = list(sorted(zip(filenames,filenames.index), key=lambda x: x[0]))\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        document_id=widgets.Dropdown(description='Document', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='50%')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "    )\n",
    "\n",
    "    def display_document_entities(corpus, document_id):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:        \n",
    "            doc = textacy_utility.get_document_by_id(corpus, document_id)\n",
    "            displacy.render(doc.spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right]),\n",
    "        widgets.VBox([gui.output], layout=widgets.Layout(margin_top='20px', height='600px',width='100%'))\n",
    "    ]))\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        document_id=gui.document_id\n",
    "    )\n",
    "    \n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_document_entities_gui(corpus, document_index=document_index)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: None, 34: None, 58: None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_entities(spacy_doc, include_types=None, drop_determiners=True):\n",
    "    \n",
    "    entities = (x for x in spacy_doc.ents if not x.text.isspace())\n",
    "    \n",
    "    if include_types is not None:\n",
    "        assert isinstance(include_types, (set, list, tuple))\n",
    "        entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiners is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "\n",
    "    for x in entities:\n",
    "        yield x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Extract Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 13:26:21,369 : INFO : Loading model en_core_web_lg\n",
      "2019-03-09 13:26:22,146 : INFO : Processing ../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip...\n",
      "2019-03-09 13:26:33,094 : INFO : Processed 100 files...671 places found...\n",
      "2019-03-09 13:26:45,063 : INFO : Processed 200 files...1451 places found...\n",
      "2019-03-09 13:26:57,933 : INFO : Processed 300 files...2302 places found...\n",
      "2019-03-09 13:27:09,577 : INFO : Processed 400 files...3191 places found...\n",
      "2019-03-09 13:27:45,338 : INFO : Processed 500 files...5008 places found...\n",
      "2019-03-09 13:28:10,292 : INFO : Processed 600 files...7182 places found...\n",
      "2019-03-09 13:28:39,495 : INFO : Processed 700 files...9474 places found...\n",
      "2019-03-09 13:29:08,556 : INFO : Processed 800 files...11563 places found...\n",
      "2019-03-09 13:29:42,103 : INFO : Processed 900 files...12990 places found...\n",
      "2019-03-09 13:30:07,988 : INFO : Processed 1000 files...14649 places found...\n",
      "2019-03-09 13:30:32,977 : INFO : Processed 1100 files...16009 places found...\n",
      "2019-03-09 13:30:51,666 : INFO : Processed 1200 files...16979 places found...\n",
      "2019-03-09 13:31:08,176 : INFO : Processed 1300 files...17895 places found...\n",
      "2019-03-09 13:31:25,733 : INFO : Processed 1400 files...18645 places found...\n",
      "2019-03-09 13:31:45,078 : INFO : Processed 1500 files...19505 places found...\n",
      "2019-03-09 13:32:00,919 : INFO : Processed 1600 files...20466 places found...\n",
      "2019-03-09 13:32:17,549 : INFO : Processed 1700 files...21398 places found...\n",
      "2019-03-09 13:32:33,110 : INFO : Processed 1800 files...22294 places found...\n",
      "2019-03-09 13:32:49,952 : INFO : Processed 1900 files...23194 places found...\n",
      "2019-03-09 13:33:11,433 : INFO : Processed 2000 files...24421 places found...\n",
      "2019-03-09 13:33:26,441 : INFO : Processed 2100 files...25252 places found...\n",
      "2019-03-09 13:33:46,116 : INFO : Processed 2200 files...26349 places found...\n",
      "2019-03-09 13:34:01,573 : INFO : Processed 2300 files...27214 places found...\n",
      "2019-03-09 13:34:19,196 : INFO : Processed 2400 files...28395 places found...\n",
      "2019-03-09 13:34:33,941 : INFO : Processed 2500 files...29230 places found...\n",
      "2019-03-09 13:34:51,478 : INFO : Processed 2600 files...30245 places found...\n",
      "2019-03-09 13:35:07,177 : INFO : Processed 2700 files...31236 places found...\n",
      "2019-03-09 13:35:23,993 : INFO : Processed 2800 files...32179 places found...\n",
      "2019-03-09 13:35:40,747 : INFO : Processed 2900 files...33076 places found...\n",
      "2019-03-09 13:35:56,886 : INFO : Processed 3000 files...34039 places found...\n",
      "2019-03-09 13:35:58,200 : INFO : Processing ../../data/francis_curated_20190326.txt_preprocessed.zip...\n",
      "2019-03-09 13:36:10,441 : INFO : Processed 100 files...1060 places found...\n",
      "2019-03-09 13:36:21,746 : INFO : Processed 200 files...2049 places found...\n",
      "2019-03-09 13:36:34,419 : INFO : Processed 300 files...3302 places found...\n",
      "2019-03-09 13:37:13,972 : INFO : Processed 400 files...5037 places found...\n",
      "2019-03-09 13:37:32,673 : INFO : Processed 500 files...6372 places found...\n",
      "2019-03-09 13:37:48,486 : INFO : Processed 600 files...7475 places found...\n",
      "2019-03-09 13:37:56,610 : INFO : Processed 700 files...7808 places found...\n",
      "2019-03-09 13:38:11,025 : INFO : Processed 800 files...8351 places found...\n",
      "2019-03-09 13:38:28,606 : INFO : Processed 900 files...9092 places found...\n",
      "2019-03-09 13:38:45,988 : INFO : Processed 1000 files...9730 places found...\n",
      "2019-03-09 13:39:03,444 : INFO : Processed 1100 files...10378 places found...\n",
      "2019-03-09 13:39:28,359 : INFO : Processed 1200 files...11114 places found...\n",
      "2019-03-09 13:39:45,036 : INFO : Processed 1300 files...11669 places found...\n",
      "2019-03-09 13:40:03,419 : INFO : Processed 1400 files...12294 places found...\n",
      "2019-03-09 13:40:18,000 : INFO : Processed 1500 files...12864 places found...\n",
      "2019-03-09 13:40:30,977 : INFO : Processed 1600 files...13434 places found...\n",
      "2019-03-09 13:40:45,148 : INFO : Processed 1700 files...14067 places found...\n",
      "2019-03-09 13:41:01,824 : INFO : Processed 1800 files...14615 places found...\n",
      "2019-03-09 13:41:18,128 : INFO : Processed 1900 files...15371 places found...\n",
      "2019-03-09 13:41:34,707 : INFO : Processed 2000 files...16062 places found...\n",
      "2019-03-09 13:41:51,490 : INFO : Processed 2100 files...16795 places found...\n",
      "2019-03-09 13:42:09,927 : INFO : Processed 2200 files...17680 places found...\n",
      "2019-03-09 13:42:33,139 : INFO : Processed 2300 files...18662 places found...\n",
      "2019-03-09 13:42:53,903 : INFO : Processed 2400 files...19528 places found...\n",
      "2019-03-09 13:43:18,592 : INFO : Processed 2500 files...20531 places found...\n",
      "2019-03-09 13:43:38,207 : INFO : Processed 2600 files...21512 places found...\n",
      "2019-03-09 13:43:58,122 : INFO : Processed 2700 files...22271 places found...\n",
      "2019-03-09 13:44:20,242 : INFO : Processed 2800 files...23187 places found...\n",
      "2019-03-09 13:44:34,633 : INFO : Processed 2900 files...23957 places found...\n"
     ]
    }
   ],
   "source": [
    "#%%file run_ner_places.py\n",
    "#from cytoolz import itertoolz\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import domain_logic_vatican as domain_logic\n",
    "import text_corpus\n",
    "import logging\n",
    "import string\n",
    "\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "logger = logging.getLogger('ner')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "NON_PRINT_CHARS = set([chr(i) for i in range(128)]).difference(string.printable)\n",
    "\n",
    "DEL_WS_CHARS = str.maketrans('', '', '\"\\n\\t')\n",
    "DEL_CRAP_CHARS = str.maketrans('', '', '\"\\n\\t?@')# + NON_PRINT_CHARS)\n",
    "#DEL_NON_PRINT = str.maketrans('', '', NON_PRINT_CHARS)\n",
    "\n",
    "#def get_doc_places(doc):\n",
    "#    return ( (w.text, w.lemma_, str(w.lemma_).translate(DEL_CRAP_CHARS), w[0].ent_type_)\n",
    "#                for w in doc.ents if w[0].ent_type_ in ['LOC', 'GPE'] and w.lemma_.strip() != '' )\n",
    "\n",
    "def get_doc_place_entities(spacy_doc, drop_determiner=True):\n",
    "    \n",
    "    DET = spacy.parts_of_speech.DET\n",
    "    PUNCT = spacy.parts_of_speech.PUNCT\n",
    "    \n",
    "    include_types = ['LOC', 'GPE']\n",
    "    \n",
    "    entities = spacy_doc.ents\n",
    "    entities = (x for x in entities if not x.text.isspace())\n",
    "    entities = (x for x in entities if x.lemma_.strip() != '')\n",
    "    entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiner is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "        \n",
    "    for x in entities:\n",
    "        text = x.text.translate(DEL_WS_CHARS)\n",
    "        lemma = x.lemma_.translate(DEL_WS_CHARS)\n",
    "        #tidy_lemma = lemma.translate(DEL_CRAP_CHARS).replace('  ', ' ')\n",
    "        tidy_lemma = lemma.replace('  ', ' ')\n",
    "        yield (text, lemma, tidy_lemma, x[0].ent_type_)\n",
    "        \n",
    "#def get_corpus_places(corpus):\n",
    "#    return itertoolz.chain.from_iterable(( get_doc_places(doc.spacy_doc) for doc in corpus ))\n",
    "\n",
    "def create_source_stream(source_path, lang, document_index=None):\n",
    "    reader = text_corpus.CompressedFileReader(source_path)\n",
    "    stream = domain_logic.get_document_stream(reader, lang, document_index=document_index)\n",
    "    return stream\n",
    "\n",
    "def create_nlp(model='en_core_web_sm', disable=None):\n",
    "    nlp = spacy.load(model, disable=disable)\n",
    "    nlp.tokenizer = textacy_utility.keep_hyphen_tokenizer(nlp)\n",
    "    return nlp\n",
    "\n",
    "source_paths = [ '../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip', '../../data/francis_curated_20190326.txt_preprocessed.zip' ]\n",
    "\n",
    "model_name = 'en_core_web_lg'\n",
    "\n",
    "logger.info('Loading model {}'.format(model_name))\n",
    "nlp = create_nlp(model=model_name, disable=('parser', 'textcat'))\n",
    "\n",
    "for source_path in source_paths:\n",
    "    logger.info('Processing {}...'.format(source_path))\n",
    "    stream = create_source_stream(source_path, 'en')\n",
    "    file_counter = 0\n",
    "    places = []\n",
    "    for filename, text, _ in stream:\n",
    "        file_counter += 1\n",
    "        doc = nlp(text)\n",
    "        places.extend(list(get_doc_place_entities(doc)))\n",
    "        if file_counter % 100 == 0:\n",
    "            logger.info('Processed {} files...{} places found...'.format(file_counter, len(places)))\n",
    "            #break\n",
    "        doc = None\n",
    "        \n",
    "df = pd.DataFrame(places, columns=['text', 'lemma', 'tidy_lemma', 'ent_type'])\n",
    "df.to_csv('./NER_with_tagging_total.txt', sep='\\t')\n",
    "\n",
    "df_grouped = df.groupby(['tidy_lemma', 'ent_type']).size().reset_index()\n",
    "\n",
    "df_grouped.to_csv('./NER_with_tagging_total_tidy_lemma_grouped.txt', sep='\\t')\n",
    "\n",
    "\n",
    "#[z for z in textacy.extract.named_entities(doc)]\n",
    "#[[ ent for ent in textacy.extract.named_entities(doc) ] for doc in corpus if len(doc.spacy_doc.ents or []) > 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nohup python3 run_ner_places.py &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Display Named Entity Statistics<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-70882e45eb62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdocument_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomain_logic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdisplay_grouped_by_entities_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-70882e45eb62>\u001b[0m in \u001b[0;36mdisplay_grouped_by_entities_gui\u001b[0;34m(corpus, document_index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_grouped_by_entities_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_named_entity_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgroup_by_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m'ent_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-70882e45eb62>\u001b[0m in \u001b[0;36mcompile_named_entity_data\u001b[0;34m(corpus, document_index, drop_determiners, min_freq)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m              for ent in textacy.extract.named_entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     ]\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-70882e45eb62>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m              for ent in textacy.extract.named_entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     ]\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-70882e45eb62>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#textacy.extract.named_entities(doc, include_types=None, exclude_types=None, drop_determiners=True, min_freq=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     data = [[\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m              for ent in textacy.extract.named_entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/textacy/extract.py\u001b[0m in \u001b[0;36mnamed_entities\u001b[0;34m(doc, include_types, exclude_types, drop_determiners, min_freq)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mnes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mne\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/textacy/extract.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         nes = (\n\u001b[1;32m    236\u001b[0m             \u001b[0mne\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDET\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mSpacySpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             for ne in nes)\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_freq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mnes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.vector.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "def compile_named_entity_data(corpus, document_index, drop_determiners=True, min_freq=1):\n",
    "    #textacy.extract.named_entities(doc, include_types=None, exclude_types=None, drop_determiners=True, min_freq=1)\n",
    "    data = [[\n",
    "        (doc.metadata['document_id'], ent[0].ent_type_, ent.text, ent.lemma_)\n",
    "             for ent in textacy.extract.named_entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n",
    "                for doc in corpus\n",
    "    ]\n",
    "    data = utility.flatten(data)\n",
    "    df = pd.DataFrame(data, columns=['document_id', 'ent_type', 'text', 'lemma']).set_index('document_id')\n",
    "    df = pd.merge(df, document_index, left_index=True, right_index=True, how='inner')\n",
    "    return df[df.year > 0][['pope', 'year', 'genre', 'ent_type', 'text', 'lemma', 'filename']].reset_index()\n",
    "\n",
    "def display_grouped_by_entities_gui(corpus, document_index):\n",
    "    \n",
    "    columns = compile_named_entity_data([corpus[0]], document_index).columns\n",
    "    \n",
    "    group_by_options = [ (x.title(), x) for x in columns if x not in [ 'ent_type', 'text', 'lemma', 'filename', 'index'] ]\n",
    "    group_by_values = [ x for _, x in group_by_options ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        group_by=widgets.SelectMultiple(description='Group by', options=group_by_options, value=group_by_values, rows=3, layout=widgets.Layout(width='180px')),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop DET',  tooltip='Drop_determiners`', icon='check'),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "    )\n",
    "        \n",
    "    def display_grouped_by_entities(corpus, group_by, drop_determiners, min_freq):\n",
    "        gui.output.clear_output()\n",
    "        named_entities = compile_named_entity_data(corpus, document_index, drop_determiners, min_freq)\n",
    "        with gui.output:\n",
    "            df = named_entities.groupby(list(group_by) + ['ent_type', 'lemma']).size().reset_index()\n",
    "            df = df.rename(columns={0:'Count'})\n",
    "            df = df.sort_values('Count', ascending=False)\n",
    "            display(df)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_grouped_by_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        #named_entities=widgets.fixed(named_entities),\n",
    "        group_by=gui.group_by,\n",
    "        drop_determiners=gui.drop_determiners,\n",
    "        min_freq=gui.min_freq\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.group_by, gui.drop_determiners, gui.min_freq]),\n",
    "        widgets.VBox([gui.output]),\n",
    "        itw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_grouped_by_entities_gui(corpus, document_index)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Stanford NER Tagger (CoreNLP)<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>PREPARE</span> Verify that Stanford CoreNLP is up and running<span style='color: green; float: right'>SKIP</span>\n",
    "Stanford CoreNLP server must be started as described in:  https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "\n",
    "With docker:\n",
    "```bash\n",
    "docker pull frnkenstien/corenlp\n",
    "docker run -p 9000:9000 --name coreNLP --rm -i -t frnkenstien/corenlp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford tagger is up and running!\n",
      " Result: Stony/ORGANIZATION Brook/ORGANIZATION University/ORGANIZATION in/O NY/STATE_OR_PROVINCE\n"
     ]
    }
   ],
   "source": [
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "try:\n",
    "    from nltk.parse import corenlp\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=STANFORD_CORE_NLP_URL, encoding='utf8', tagtype='ner')\n",
    "    input_tokens = 'Stony Brook University in NY'.split()\n",
    "    tagged_output = corenlp_tagger.tag(input_tokens)\n",
    "    print('Stanford tagger is up and running!')\n",
    "    print(' Result: ' + ' '.join([ x + '/' + y for x,y in tagged_output]))\n",
    "except: # (ConnectionError, ConnectionRefusedError):\n",
    "    logger.error('Server not found! Please start Stanford CoreNLP Server!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ca025de543416a9cf13742cd0d4c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Corpus', index=3, layout=Layout(width='300p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import text_corpus\n",
    "import domain_logic_vatican as domain_logic\n",
    "import nltk\n",
    "\n",
    "from nltk.parse import corenlp\n",
    "\n",
    "def merge_entities(entities):\n",
    "    n_entities = len(entities)\n",
    "    if n_entities <= 1:\n",
    "        return entities\n",
    "    merged = entities[:1]\n",
    "    for i_n, w_n, t_n in entities[1:]:\n",
    "        i_p, w_p, t_p = merged[-1]\n",
    "        if i_n == i_p + 1 and t_n == t_p:\n",
    "            merged[-1] = (i_n, '_'.join([w_p, w_n]), t_p)\n",
    "        else:\n",
    "            merged.append((i_n, w_n, t_n))\n",
    "    return merged\n",
    "\n",
    "def recognize_named_entities(tagger, doc_id, text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    start_index = 0\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        ents = [ (start_index + index, word, ent_type) for index, (word, ent_type) in enumerate(tagger.tag(tokens)) if ent_type not in excludes ]\n",
    "        start_index += len(sentence)\n",
    "    merged_ents = merge_entities(ents)\n",
    "        \n",
    "def compute_stanford_ner(data_folder, source_file, service_url=STANFORD_CORE_NLP_URL, excludes=None):\n",
    "    \n",
    "    excludes = excludes or ('O', )\n",
    "    \n",
    "    assert os.path.isfile(source_file), 'File missing!'\n",
    "    \n",
    "    tagger = corenlp.CoreNLPParser(url=service_url, encoding='utf8', tagtype='ner')\n",
    "    \n",
    "    reader = text_corpus.CompressedFileReader(source_file)\n",
    "    document_index = domain_logic.compile_documents_by_filename(reader.filenames)\n",
    "    stream = domain_logic.get_document_stream(reader, 'en', document_index=document_index)\n",
    "    for filename, text, metadata in stream:\n",
    "        document_id = document_index[document_index.filename == filename].document_id\n",
    "        ner_data = recognize_named_entities(tagger, text)\n",
    "        \n",
    "def display_stanford_ner_gui(data_folder):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus_files = sorted(glob.glob(os.path.join(data_folder, '*.txt.zip')))\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        source_path=widgets_config.dropdown(description='Corpus', options=corpus_files, value=corpus_files[-1], layout=lw('300px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('100px'))\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.source_path,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.compute,\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def compute_stanford_ner_callback(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            compute_stanford_ner(\n",
    "                data_folder=widgets.fixed(data_folder),\n",
    "                source_file=gui.source_path.value\n",
    "            )\n",
    "            \n",
    "    gui.compute.on_click(compute_stanford_ner_callback)\n",
    "\n",
    "data_folder = '../../data'\n",
    "display_stanford_ner_gui(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, codecs, time, re, collections, zipfile\n",
    "\n",
    "def extract_entity_phrases(data, classes=[ 'LOCATION', 'PERSON']):\n",
    "\n",
    "    # Extract entities of selected classes, add index to enable merge to phrases\n",
    "    entities = [ (i, word, wclass)\n",
    "        for (i, (word, wclass)) in enumerate(data) if classes is None or wclass in classes ]\n",
    "\n",
    "    # Merge adjacent entities having the same classifier\n",
    "    for i in range(len(entities) - 1, 0, -1):\n",
    "        if entities[i][0] == entities[i - 1][0] + 1 and entities[i][2] == entities[i - 1][2]:\n",
    "            entities[i - 1] = (entities[i - 1][0], entities[i - 1][1] + \" \" + entities[i][1], entities[i - 1][2])\n",
    "            del entities[i]\n",
    "\n",
    "    # Remove index in returned data\n",
    "    return [ (word, wclass) for (i, word, wclass) in entities  ]\n",
    "\n",
    "def create_ner_tagger(options):\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8', tagtype='ner')\n",
    "    return corenlp_tagger\n",
    "\n",
    "def create_tokenizer(options):\n",
    "    corenlp_tokenizer = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8')\n",
    "    return corenlp_tokenizer\n",
    "\n",
    "def create_statistics(entities):\n",
    "    wc = collections.Counter()\n",
    "    wc.update(entities)\n",
    "    return wc\n",
    "\n",
    "def serialize_content(stats, filename, token_count):\n",
    "    document_name, treaty_id, lang = extract_document_info(filename)\n",
    "    data = [ (document_name, treaty_id, lang, word, wclass, stats[(word, wclass)], token_count) for (word, wclass) in stats  ]\n",
    "    content = '\\n'.join(map(lambda x: ';'.join([str(y) for y in x]), data))\n",
    "    return content\n",
    "\n",
    "def write_content(outfile, content):\n",
    "    if content != '':\n",
    "        outfile.write(content)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "def recognize_entities(options):\n",
    "\n",
    "    corenlp_tokenizer = create_tokenizer(options)\n",
    "    corenlp_tagger = create_ner_tagger(options)\n",
    "    \n",
    "    outfile = os.path.join(options['output_folder'], \"output_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\")\n",
    "    tags = [ 'NUMBER', 'LOCATION', 'DATE', 'MISC', 'ORGANIZATION', 'DURATION', 'SET', 'ORDINAL', 'PERSON' ]\n",
    "    \n",
    "    document_stream = treaty_corpus.get_document_stream(options['source_path'], options['language'], treaties)\n",
    "    for treaty_id, language, filename, content in document_stream:\n",
    "        print('treaty_id')\n",
    "        \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"../data/test_corpora_preprocessed.zip\",\n",
    "    'server_url': STANFORD_CORE_NLP_URL,\n",
    "    'output_folder': DATA_FOLDER,\n",
    "}\n",
    "\n",
    "recognize_entities(options)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
