{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os #, collections, zipfile\n",
    "#import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import domain_logic_vatican as domain_logic\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER, PATTERN = '../../data',  '*.txt'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set_matplotlib_formats('svg')   \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    # FIXME VARYING ASPECTS: document_index = WTI_INDEX for tCoIR\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, document_index=None, container=container, compute_ner=True, domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, document_index):\n",
    "    \n",
    "    # FIXME VARYING ASPECT: Add \"document_name\" to document_index, or function that creates name\n",
    "    filenames = document_index.filename\n",
    "    document_options = list(sorted(zip(filenames,filenames.index), key=lambda x: x[0]))\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        document_id=widgets.Dropdown(description='Document', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='50%')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=widgets.Layout(width='40px')),\n",
    "    )\n",
    "\n",
    "    def display_document_entities(corpus, document_id):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:        \n",
    "            doc = textacy_utility.get_document_by_id(corpus, document_id)\n",
    "            displacy.render(doc.spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right]),\n",
    "        widgets.VBox([gui.output], layout=widgets.Layout(margin_top='20px', height='600px',width='100%'))\n",
    "    ]))\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        document_id=gui.document_id\n",
    "    )\n",
    "    \n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_document_entities_gui(corpus, document_index=document_index)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(spacy_doc, include_types=None, drop_determiners=True):\n",
    "    \n",
    "    entities = (x for x in spacy_doc.ents if not x.text.isspace())\n",
    "    \n",
    "    if include_types is not None:\n",
    "        assert isinstance(include_types, (set, list, tuple))\n",
    "        entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiners is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "\n",
    "    for x in entities:\n",
    "        yield x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Extract Named Entities<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file run_ner_places.py\n",
    "#from cytoolz import itertoolz\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import domain_logic_vatican as domain_logic\n",
    "import text_corpus\n",
    "import logging\n",
    "import string\n",
    "\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "\n",
    "logger = logging.getLogger('ner')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "NON_PRINT_CHARS = set([chr(i) for i in range(128)]).difference(string.printable)\n",
    "\n",
    "DEL_WS_CHARS = str.maketrans('', '', '\"\\n\\t')\n",
    "DEL_CRAP_CHARS = str.maketrans('', '', '\"\\n\\t?@')# + NON_PRINT_CHARS)\n",
    "#DEL_NON_PRINT = str.maketrans('', '', NON_PRINT_CHARS)\n",
    "\n",
    "#def get_doc_places(doc):\n",
    "#    return ( (w.text, w.lemma_, str(w.lemma_).translate(DEL_CRAP_CHARS), w[0].ent_type_)\n",
    "#                for w in doc.ents if w[0].ent_type_ in ['LOC', 'GPE'] and w.lemma_.strip() != '' )\n",
    "\n",
    "def get_doc_place_entities(spacy_doc, drop_determiner=True):\n",
    "    \n",
    "    DET = spacy.parts_of_speech.DET\n",
    "    PUNCT = spacy.parts_of_speech.PUNCT\n",
    "    \n",
    "    include_types = ['LOC', 'GPE']\n",
    "    \n",
    "    entities = spacy_doc.ents\n",
    "    entities = (x for x in entities if not x.text.isspace())\n",
    "    entities = (x for x in entities if x.lemma_.strip() != '')\n",
    "    entities = (x for x in entities if x.label_ in include_types)\n",
    "\n",
    "    if drop_determiner is True:\n",
    "        entities = (x if x[0].pos != DET else SpacySpan(x.doc, x.start + 1, x.end, label=x.label, vector=x.vector) for x in entities)\n",
    "        \n",
    "    for x in entities:\n",
    "        text = x.text.translate(DEL_WS_CHARS)\n",
    "        lemma = x.lemma_.translate(DEL_WS_CHARS)\n",
    "        #tidy_lemma = lemma.translate(DEL_CRAP_CHARS).replace('  ', ' ')\n",
    "        tidy_lemma = lemma.replace('  ', ' ')\n",
    "        yield (text, lemma, tidy_lemma, x[0].ent_type_)\n",
    "        \n",
    "#def get_corpus_places(corpus):\n",
    "#    return itertoolz.chain.from_iterable(( get_doc_places(doc.spacy_doc) for doc in corpus ))\n",
    "\n",
    "def create_source_stream(source_path, lang, document_index=None):\n",
    "    reader = text_corpus.CompressedFileReader(source_path)\n",
    "    stream = domain_logic.get_document_stream(reader, lang, document_index=document_index)\n",
    "    return stream\n",
    "\n",
    "def create_nlp(model='en_core_web_sm', disable=None):\n",
    "    nlp = spacy.load(model, disable=disable)\n",
    "    nlp.tokenizer = textacy_utility.keep_hyphen_tokenizer(nlp)\n",
    "    return nlp\n",
    "\n",
    "source_paths = [ '../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip', '../../data/francis_curated_20190326.txt_preprocessed.zip' ]\n",
    "\n",
    "model_name = 'en_core_web_lg'\n",
    "\n",
    "logger.info('Loading model {}'.format(model_name))\n",
    "nlp = create_nlp(model=model_name, disable=('parser', 'textcat'))\n",
    "\n",
    "for source_path in source_paths:\n",
    "    logger.info('Processing {}...'.format(source_path))\n",
    "    stream = create_source_stream(source_path, 'en')\n",
    "    file_counter = 0\n",
    "    places = []\n",
    "    for filename, text, _ in stream:\n",
    "        file_counter += 1\n",
    "        doc = nlp(text)\n",
    "        places.extend(list(get_doc_place_entities(doc)))\n",
    "        if file_counter % 100 == 0:\n",
    "            logger.info('Processed {} files...{} places found...'.format(file_counter, len(places)))\n",
    "            #break\n",
    "        doc = None\n",
    "        \n",
    "df = pd.DataFrame(places, columns=['text', 'lemma', 'tidy_lemma', 'ent_type'])\n",
    "df.to_csv('./NER_with_tagging_total.txt', sep='\\t')\n",
    "\n",
    "df_grouped = df.groupby(['tidy_lemma', 'ent_type']).size().reset_index()\n",
    "\n",
    "df_grouped.to_csv('./NER_with_tagging_total_tidy_lemma_grouped.txt', sep='\\t')\n",
    "\n",
    "\n",
    "#[z for z in textacy.extract.named_entities(doc)]\n",
    "#[[ ent for ent in textacy.extract.named_entities(doc) ] for doc in corpus if len(doc.spacy_doc.ents or []) > 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nohup python3 run_ner_places.py &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Display Named Entity Statistics<span style='color: green; float: right'>TRY IT</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_named_entity_data(corpus, document_index, drop_determiners=True, min_freq=1):\n",
    "    #textacy.extract.named_entities(doc, include_types=None, exclude_types=None, drop_determiners=True, min_freq=1)\n",
    "    data = [[\n",
    "        (doc.metadata['document_id'], ent[0].ent_type_, ent.text, ent.lemma_)\n",
    "             for ent in textacy.extract.named_entities(doc, exclude_types=('CARDINAL',), drop_determiners=drop_determiners, min_freq=min_freq) ]\n",
    "                for doc in corpus\n",
    "    ]\n",
    "    data = utility.flatten(data)\n",
    "    df = pd.DataFrame(data, columns=['document_id', 'ent_type', 'text', 'lemma']).set_index('document_id')\n",
    "    df = pd.merge(df, document_index, left_index=True, right_index=True, how='inner')\n",
    "    return df[df.year > 0][['pope', 'year', 'genre', 'ent_type', 'text', 'lemma', 'filename']].reset_index()\n",
    "\n",
    "def display_grouped_by_entities_gui(corpus, document_index):\n",
    "    \n",
    "    columns = compile_named_entity_data([corpus[0]], document_index).columns\n",
    "    \n",
    "    group_by_options = [ (x.title(), x) for x in columns if x not in [ 'ent_type', 'text', 'lemma', 'filename', 'index'] ]\n",
    "    group_by_values = [ x for _, x in group_by_options ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        group_by=widgets.SelectMultiple(description='Group by', options=group_by_options, value=group_by_values, rows=3, layout=widgets.Layout(width='180px')),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop DET',  tooltip='Drop_determiners`', icon='check'),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        min_freq=widgets.IntSlider(description='Min freq', min=1, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "    )\n",
    "        \n",
    "    def display_grouped_by_entities(corpus, group_by, drop_determiners, min_freq):\n",
    "        gui.output.clear_output()\n",
    "        named_entities = compile_named_entity_data(corpus, document_index, drop_determiners, min_freq)\n",
    "        with gui.output:\n",
    "            df = named_entities.groupby(list(group_by) + ['ent_type', 'lemma']).size().reset_index()\n",
    "            df = df.rename(columns={0:'Count'})\n",
    "            df = df.sort_values('Count', ascending=False)\n",
    "            display(df)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_grouped_by_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        #named_entities=widgets.fixed(named_entities),\n",
    "        group_by=gui.group_by,\n",
    "        drop_determiners=gui.drop_determiners,\n",
    "        min_freq=gui.min_freq\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.group_by, gui.drop_determiners, gui.min_freq]),\n",
    "        widgets.VBox([gui.output]),\n",
    "        itw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    document_index = domain_logic.compile_documents(corpus)\n",
    "    display_grouped_by_entities_gui(corpus, document_index)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Stanford NER Tagger (CoreNLP)<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>PREPARE</span> Verify that Stanford CoreNLP is up and running<span style='color: green; float: right'>SKIP</span>\n",
    "Stanford CoreNLP server must be started as described in:  https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "\n",
    "With docker:\n",
    "```bash\n",
    "docker pull frnkenstien/corenlp\n",
    "docker run -p 9000:9000 --name coreNLP --rm -i -t frnkenstien/corenlp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford tagger is up and running!\n",
      " Result: Stony/ORGANIZATION Brook/ORGANIZATION University/ORGANIZATION in/O NY/STATE_OR_PROVINCE\n"
     ]
    }
   ],
   "source": [
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "try:\n",
    "    from nltk.parse import corenlp\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=STANFORD_CORE_NLP_URL, encoding='utf8', tagtype='ner')\n",
    "    input_tokens = 'Stony Brook University in NY'.split()\n",
    "    tagged_output = corenlp_tagger.tag(input_tokens)\n",
    "    print('Stanford tagger is up and running!')\n",
    "    print(' Result: ' + ' '.join([ x + '/' + y for x,y in tagged_output]))\n",
    "except: # (ConnectionError, ConnectionRefusedError):\n",
    "    logger.error('Server not found! Please start Stanford CoreNLP Server!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_stanford_ner.py\n"
     ]
    }
   ],
   "source": [
    "%%file run_stanford_ner.py\n",
    "import os, sys, time\n",
    "import glob\n",
    "import types\n",
    "import ipywidgets as widgets\n",
    "import text_corpus\n",
    "import domain_logic_vatican as domain_logic\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import common.widgets_config as widgets_config\n",
    "import common.utility as utility\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "from nltk.parse import corenlp\n",
    "\n",
    "STANFORD_CORE_NLP_URL = 'http://localhost:9000'\n",
    "\n",
    "def merge_entities(entities):\n",
    "    n_entities = len(entities)\n",
    "    if n_entities <= 1:\n",
    "        return entities\n",
    "    merged = entities[:1]\n",
    "    for doc_id, i_n, w_n, t_n in entities[1:]:\n",
    "        doc_id_p, i_p, w_p, t_p = merged[-1]\n",
    "        if i_n == i_p + 1 and t_n == t_p:\n",
    "            merged[-1] = (doc_id, i_n, '_'.join([w_p, w_n]), t_p)\n",
    "        else:\n",
    "            merged.append((doc_id, i_n, w_n, t_n))\n",
    "    return merged\n",
    "\n",
    "def recognize_named_entities(tagger, doc_id, text, excludes=None):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    start_index = 0\n",
    "    excludes = excludes or []\n",
    "    merged_ents = []\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        ents = [ (doc_id, start_index + index, word, ent_type) for index, (word, ent_type) in enumerate(tagger.tag(tokens)) if ent_type not in excludes ]\n",
    "        start_index += len(sentence)\n",
    "        merged_ents = merge_entities(ents)\n",
    "    return merged_ents\n",
    "\n",
    "#\n",
    "#     if not os.path.isfile(targetfile) or overwrite:\n",
    "#         if os.path.isfile(targetfile):\n",
    "#             os.remove(targetfile)\n",
    "#         with zipfile.ZipFile(targetfile, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "#             pass\n",
    "#     else:\n",
    "        \n",
    "#         with zipfile.ZipFile(targetfile, \"r\") as z:\n",
    "#             zip_filenames = z.namelist()\n",
    "            \n",
    "#         pdf_filepaths = list(set(pdf_filepaths) - set(zip_filenames))\n",
    "    \n",
    "#     with zipfile.ZipFile(targetfile, \"a\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "#         z.writestr(text_filename, pdf_text)\n",
    "            \n",
    "def compute_stanford_ner(source_file, service_url=STANFORD_CORE_NLP_URL, excludes=None):\n",
    "    \n",
    "    excludes = excludes or ('O', )\n",
    "    \n",
    "    assert os.path.isfile(source_file), 'File missing!'\n",
    "    \n",
    "    tagger = corenlp.CoreNLPParser(url=service_url, encoding='utf8', tagtype='ner')\n",
    "    \n",
    "    reader = text_corpus.CompressedFileReader(source_file)\n",
    "    document_index = domain_logic.compile_documents_by_filename(reader.filenames)\n",
    "    stream = domain_logic.get_document_stream(reader, 'en', document_index=document_index)\n",
    "    \n",
    "    i = 0\n",
    "    ner_data = []\n",
    "    for filename, text, metadata in stream:\n",
    "        document_id = document_index.loc[document_index.filename == filename, 'document_id'].values[0]\n",
    "        ner = recognize_named_entities(tagger, document_id, text, excludes)\n",
    "        ner_data.extend(ner)\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            logger.info('Processed {} files...'.format(i))\n",
    "            #break\n",
    "    return ner_data\n",
    "\n",
    "def compute_and_store_stanford_ner(source_file):\n",
    "    \n",
    "    ner_data = compute_stanford_ner(source_file=source_file)\n",
    "    df = pd.DataFrame(ner_data, columns=['doc_id', 'pos', 'entity', 'ent_type'])\n",
    "    df.index.name = 'id'\n",
    "\n",
    "    store_name = 'ner_{}_{}.txt'.format(\n",
    "        os.path.splitext(os.path.split(source_file)[1])[0],\n",
    "        time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "    )\n",
    "\n",
    "    df.to_csv(store_name, sep='\\t')\n",
    "\n",
    "    logger.info('Result stored in %s', store_name)\n",
    "    return df\n",
    "        \n",
    "def display_stanford_ner_gui(data_folder):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus_files = sorted(glob.glob(os.path.join(data_folder, '*.txt.zip')))\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        source_path=widgets_config.dropdown(description='Corpus', options=corpus_files, value=corpus_files[-1], layout=lw('300px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('100px'))\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.source_path,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.compute,\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def compute_stanford_ner_callback(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            compute_and_store_stanford_ner(gui.source_path.value)\n",
    "            #display(df)\n",
    "    gui.compute.on_click(compute_stanford_ner_callback)\n",
    "\n",
    "\n",
    "#data_folder = '../../data'\n",
    "#display_stanford_ner_gui(data_folder)\n",
    "for source_file in [ '../../data/benedict-xvi_curated_20190326.txt.zip', '../../data/francis_curated_20190326.txt.zip' ]:\n",
    "    compute_and_store_stanford_ner(source_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benedict-xvi_curated_20190326.txt_preprocessed', '.zip')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../../data/benedict-xvi_curated_20190326.txt_preprocessed.zip'\n",
    "os.path.splitext(os.path.split(filename)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, codecs, time, re, collections, zipfile\n",
    "\n",
    "def extract_entity_phrases(data, classes=[ 'LOCATION', 'PERSON']):\n",
    "\n",
    "    # Extract entities of selected classes, add index to enable merge to phrases\n",
    "    entities = [ (i, word, wclass)\n",
    "        for (i, (word, wclass)) in enumerate(data) if classes is None or wclass in classes ]\n",
    "\n",
    "    # Merge adjacent entities having the same classifier\n",
    "    for i in range(len(entities) - 1, 0, -1):\n",
    "        if entities[i][0] == entities[i - 1][0] + 1 and entities[i][2] == entities[i - 1][2]:\n",
    "            entities[i - 1] = (entities[i - 1][0], entities[i - 1][1] + \" \" + entities[i][1], entities[i - 1][2])\n",
    "            del entities[i]\n",
    "\n",
    "    # Remove index in returned data\n",
    "    return [ (word, wclass) for (i, word, wclass) in entities  ]\n",
    "\n",
    "def create_ner_tagger(options):\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8', tagtype='ner')\n",
    "    return corenlp_tagger\n",
    "\n",
    "def create_tokenizer(options):\n",
    "    corenlp_tokenizer = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8')\n",
    "    return corenlp_tokenizer\n",
    "\n",
    "def create_statistics(entities):\n",
    "    wc = collections.Counter()\n",
    "    wc.update(entities)\n",
    "    return wc\n",
    "\n",
    "def serialize_content(stats, filename, token_count):\n",
    "    document_name, treaty_id, lang = extract_document_info(filename)\n",
    "    data = [ (document_name, treaty_id, lang, word, wclass, stats[(word, wclass)], token_count) for (word, wclass) in stats  ]\n",
    "    content = '\\n'.join(map(lambda x: ';'.join([str(y) for y in x]), data))\n",
    "    return content\n",
    "\n",
    "def write_content(outfile, content):\n",
    "    if content != '':\n",
    "        outfile.write(content)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "def recognize_entities(options):\n",
    "\n",
    "    corenlp_tokenizer = create_tokenizer(options)\n",
    "    corenlp_tagger = create_ner_tagger(options)\n",
    "    \n",
    "    outfile = os.path.join(options['output_folder'], \"output_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\")\n",
    "    tags = [ 'NUMBER', 'LOCATION', 'DATE', 'MISC', 'ORGANIZATION', 'DURATION', 'SET', 'ORDINAL', 'PERSON' ]\n",
    "    \n",
    "    document_stream = treaty_corpus.get_document_stream(options['source_path'], options['language'], treaties)\n",
    "    for treaty_id, language, filename, content in document_stream:\n",
    "        print('treaty_id')\n",
    "        \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"../data/test_corpora_preprocessed.zip\",\n",
    "    'server_url': STANFORD_CORE_NLP_URL,\n",
    "    'output_folder': DATA_FOLDER,\n",
    "}\n",
    "\n",
    "recognize_entities(options)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
